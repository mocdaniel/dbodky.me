{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"dbodky.me","text":"Hi, I'm Daniel <p>Developer Advocate &amp; Cloud-Native Educator</p> <p>I break things, fix them, and share the lessons along the way.</p> My Blog Get In Touch Scroll down to explore more content About Me <p>My passion lies in breaking down complex technological concepts to help practictioners adopt modern cloud practices.</p> Kubernetes Cloud-Native DevOps GitOps Technical Writing Public Speaking Trainings Read More on My Blog Get in Touch <p>Have questions or want to collaborate? I'm always open to discussing cloud-native technologies and collaboration.</p> Schedule a Call <p>Book a time slot that works for you to discuss your project or ideas.</p> Schedule a Call Email Me <p>Reach out directly with your questions or collaboration ideas.</p> Email Me Connect With Me <p>You can find me on many social platforms - links are in the footer.</p>"},{"location":"about/","title":"About Me","text":"About Me <p>Developer Advocate &amp; Cloud-Native Educator</p> Hello, I'm Daniel <p>I'm a 27 years old Platform Advocate from Southern Germany. I work for NETWAYS Web Services in Nuremberg, where I\u2019m advocating (not only!) for our cloudnative products and services.</p> <p>I\u2019ve got a strong interest in Kubernetes, Automation - especially GitOps, tools helping with developer experience, and container technologies. Sometimes I write about those topics or give talks at conferences.</p> <p>In my freetime, I enjoy reading High Fantasy and SciFi, help with running a LUG (LEGO User Group), and go on hikes or gravel bike tours around the area.</p> <p>You can find me on GitHub, Twitter, and LinkedIn if you want to contact me or got questions. Just follow the social links on the right - I\u2019m always happy to get in touch with new folks.</p> Connect                LinkedIn                             GitHub              Bluesky               Bluesky              Expertise &amp; Achievements Kubestronaut <p>One of 77 people in Germany and 1357 people worldwide (March 2025) holding the CKA, CKAD, CKS, KCNA, and KCSA certifications focusing on different areas of the Kubernetes cosmos.</p> Open Source Contributor <p>Active contributor to several open-source projects, strong advocate for the Educates Training Platform and Material for MKDocs Community Expert.</p> LiFT Scholarship Recipient <p>In the Cloud Captain category, back in 2022. The Linux Foundation Training Scholarship is issued to 500 aspiring individuals in the cloud-native space each year.</p> Recent Speaking Engagements NOV 2024 Open Source Monitoring Conference 2024 Reimagining Technical Training With Educates                Slides                             Video              JUL 2024 Kubernetes Community Days Munich 2024 Towards Standardized Platforms: How the CNOE Project Can Help                Slides                             Video              MAR 2024 DevOpsDays Zurich 2024 ArgoCD Odyssey: Navigating from Basics to Brilliance Let's Connect <p>Interested in having me speak at your event, contribute to your project, or discuss cloud-native technologies?</p> Get in Touch Email Me Directly"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/cloud-native-rejekts-eu24-preview/","title":"Cloud-Native Rejekts EU'24 Preview","text":"<p>Cloud-Native Rejekts EU'24 is around the corner and I prepared my schedule. Read on to see what I'll be focusing on, and get a first impression of the predominant topics.</p> <p>Paris 2024 will mark my third KubeCon, and my first time at Rejekts to go along with it. In order to not get completely overwhelmed by two conferences back to back, I took a good look at the conferences' schedules over the weekend, and identified a few talks and larger topics I will be focusing on:</p> <ul> <li>Kubernetes Networking and the Gateway API</li> <li>Platforms built on Kubernetes</li> <li>Kubernetes Deep Dives</li> </ul> <p>Let's have a look at a few talks from those topics, and why I picked them.</p>"},{"location":"blog/cloud-native-rejekts-eu24-preview/#kubernetes-networking-and-the-gateway-api","title":"Kubernetes Networking and the Gateway API","text":"<p>At Rejekts alone, there will be three talks about the Kubernetes Gateway API - people want to move on from Kubernetes Ingress resources, it seems. The Gateway API represents the next generation of Kubernetes Ingress, Load Balancing, and Service Mesh APIs<sup>1</sup> and aims at being generic, expressive, and role-oriented.</p> <p>What\u2019s new in the Kubernetes Gateway API (Link) by Abdelfettah Sghiouar will give an overview of the current state of the Kubernetes Gateway API and its implementations and provide an opportunity to get updated on the project. Abdel is one of the hosts of the Kubernetes Podcast from Google, and I like his way of breaking down large or complex topics.</p> <p>Once updated on the current state of the Gateway API, I am looking forward to Unlocking the Gateway: A Practical Guide from Ingress to Gateway API (Link), where Lior Lieberman and Mattia Lavacca will demonstrate how to migrate from Ingress to Gateway API using ingress2gateway. Both speakers are contributors to the Gateway API project, and I think we will be given very interesting insights about the Kubernetes Gateway API throughout the talk.</p>"},{"location":"blog/cloud-native-rejekts-eu24-preview/#platforms-built-on-kubernetes","title":"Platforms built on Kubernetes","text":"<p>The next topic I'm interested in revolves around platforms built on Kubernetes (I go by the title of Platform Advocate after all). We saw the rise of Internal Developer Platforms (IDPs) over the last year(s), and looking at conference schedules, people are still keen on building platforms!</p> <p>Choose Your Own Adventure: The Perilous Passage to Production is an interactive session about going from dev to prod on Kubernetes, presented by Whitney Lee and Viktor Farcic, who ran a previous rendition at KubeCon EU 2023 already (Recording on YouTube). I missed it back then, so I'm dead-set on making it this time!</p> <p>While it's not a platform the audience will be building, it's a still a production cluster, with all the building blocks and decisions you'd probably be faced with when assembling a platform - close enough for me.</p> <p>If you are looking for a talk a bit more focused on delivering platforms to clients, No GitOps Pain, No Platform gain: Day 2 Challenges of Managing Kubernetes Fleets with GitOps (Link) might be right up your alley. \u0141ukasz Pi\u0105tkowski will talk about lived experiences with platform delivery to a multitude of clients, and the stories you'd expect in such a setting.</p>"},{"location":"blog/cloud-native-rejekts-eu24-preview/#kubernetes-deep-dives","title":"Kubernetes Deep Dives","text":"<p>As mentioned earlier, KubeCon 2024 will mark my third rendition, and almost exactly two years of working with Kubernetes. It won't come as a surprise that I haven't seen nearly all of the fancy things you can pull off with one of the largest Open-Source projects in the world.</p> <p>And that's exactly why I try to catch a few talks highlighting some of the more hidden knobs and levers of Kubernetes which I haven't heard of before or at least haven't used myself yet.</p> <p>At Rejekts, these talks will be Don\u2019t Do What Charlie Don\u2019t Does - Avoiding Common CRD Design errors (Link) by Nick Young, From Fragile to Resilient: ValidatingAdmissionPolicies Strengthen Kubernetes (Link) by Marcus Noble, and The Storage Crashcourse - From CSI to Databases (Link) by Benjamin Ritter - mainly because deploying and configuring Rook, Ceph Clusters, and CSI snapshots in my homelab has been a humbling experience recently.</p>"},{"location":"blog/cloud-native-rejekts-eu24-preview/#see-you-at-rejekts","title":"See you at Rejekts?","text":"<p>This is my personal list of talks to definitely attend, and who knows where I'll end up spontaneously over the course of the two days at Rejekts 2024.</p> <p>Which talks are you going to see? If you haven't had a look at the schedule yet, now's the time! If you're attending, feel free to let me know, I'd love to have a chat - the hallway track is the most interesting one, after all.</p> <ol> <li> <p>Taken from the Introduction to Gateway API by SIG Network.\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/how-to-reverse-proxy-applications-on-subpaths-with-traefik/","title":"How to Reverse-Proxy Applications on Subpaths with Traefik","text":"<p>Reverse-proxying applications on subpaths with Traefik should be straight-forward, but sometimes it isn't. Here's how to do it anyways.</p> <p>One of my goals for 2024 is to get my homelab back up and running. Since I want to be able to access my applications on easy-to-remember domains, I use Traefik as a reverse-proxy. This allows me to access my applications on subdomains or paths, like this:</p> <ul> <li><code>https://prometheus.dbodky.me</code></li> <li><code>https://dbodky.me/grafana</code></li> </ul> <p>While the former is easy to set up, the latter can be a bit tricky sometimes. In this post, I'll explain why and show you how to set up Traefik to reverse-proxy applications on subpaths.</p>"},{"location":"blog/how-to-reverse-proxy-applications-on-subpaths-with-traefik/#the-problem","title":"The Problem","text":"<p>To understand why it can be problematic to reverse-proxy applications on a subpath sometimes, let's walk through the scenario together, with the example above:</p> <p><code>https://dbodky.me/grafana</code></p> <p>The following compose file can be spun up using <code>docker compose up -d</code> and will create two containers, one for Traefik and one for Grafana:</p> <pre><code>services:\n  traefik:\n    image: traefik:latest\n    container_name: traefik\n    command:\n      - --api.insecure=true\n      - --api.dashboard=true\n      - --providers.docker=true\n      - --entrypoints.web.address=:80\n    ports:\n      - 80:80\n      - 8080:8080\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n\n  grafana:\n    labels:\n      - traefik.http.routers.grafana.rule=PathPrefix(`/grafana`)\n      - traefik.http.routers.grafana.entrypoints=web\n      - traefik.enable=true\n    image: grafana/grafana:latest\n    ports:\n      - 3000:3000\n    container_name: grafana\n</code></pre> <p>If you navigate to http://localhost:8080/dashboard/#/http/routers/grafana@docker, you'll see that Traefik has created a router for Grafana, according to the labels we've set in the compose file:</p> Traefik created a router for Grafana at `/grafana` <p>Let's try connecting to Grafana by navigating to http://localhost/grafana. Unfortunately, this doesn't work. Instead, we get redirected to <code>http://localhost/login</code> and a 404 page.</p> <p>Just to make sure, let's try connecting to Grafana directly at http://localhost:3000. This works, Grafana is running and accessible. So what's the problem?</p>"},{"location":"blog/how-to-reverse-proxy-applications-on-subpaths-with-traefik/#running-applications-on-configurable-subpaths","title":"Running Applications on Configurable Subpaths","text":"<p>Grafana is not aware of the fact that it's being reverse-proxied. When we navigate to <code>http://localhost/grafana</code>, Grafana thinks that it's being accessed at the root path (<code>/</code>), and redirects us to its login page at <code>/login</code>.</p> <p>This redirect gets reverse-proxied by Traefik again, but there's no rule for <code>/login</code> - we get served a <code>404</code> error.</p> Request flows in our compose stack <p>Luckily, many applications allow us to configure the root path they're being accessed at. In Grafana's case, we can add the following block to its <code>service</code> definition in our compose file:</p> <pre><code>environment:\n  - GF_SERVER_ROOT_URL=%(protocol)s://%(domain)s:%(http_port)s/grafana\n  - GF_SERVER_SERVE_FROM_SUB_PATH=true\n</code></pre> <p>After restarting the stack by issuing <code>docker compose up -d</code> again, we can now access Grafana at http://localhost/grafana and it works as expected.</p> <p>But how do we deal with applications that don't allow us to configure the root path?</p>"},{"location":"blog/how-to-reverse-proxy-applications-on-subpaths-with-traefik/#running-arbitrary-applications-on-subpaths","title":"Running Arbitrary Applications on Subpaths","text":"<p>The reason I am writing this post is because I was trying to set up cAdvisor, a tool for monitoring Docker containers, on a subpath. Unfortunately, cAdvisor doesn't allow us to configure the root path, so I ran into the same problem as above.</p> <p>Let's add cAdvisor to our compose file and see what happens when we try to access it at http://localhost/cadvisor:</p> <pre><code>services:\n  [...]\n  cadvisor:\n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.http.routers.cadvisor.rule=PathPrefix(`/cadvisor`)\"\n      - \"traefik.http.routers.cadvisor.entrypoints=web\"\n    image: gcr.io/cadvisor/cadvisor:v0.47.2\n    container_name: cadvisor\n    volumes:\n      - /:/rootfs:ro\n      - /var/run:/var/run:ro\n      - /sys:/sys:ro\n      - /var/lib/docker/:/var/lib/docker:ro\n      - /dev/disk/:/dev/disk:ro\n    devices:\n      - /dev/kmsg\n    privileged: true\n</code></pre> <p>As expected, we get served a <code>404</code> error, because cAdvisor tries to redirect us to <code>/containers/</code>, apparently. So how do we fix this?</p>"},{"location":"blog/how-to-reverse-proxy-applications-on-subpaths-with-traefik/#the-solutions","title":"The Solution(s)","text":"<p>As cAdvisor doesn't allow us to configure the root path, we have to find a different solution to our problem. The obvious one relies on Traefik's <code>StripPrefix</code> middleware. It works like this:</p> <ol> <li>We hit Traefik with a request to <code>/cadvisor</code></li> <li>Traefik determines that the request should be routed to cAdvisor</li> <li>Traefik strips the prefix <code>/cadvisor</code> from the request before forwarding it to cAdvisor</li> </ol> <p>We can add the following label to cAdvisor's service definition to enable the <code>StripPrefix</code> middleware and check the result in Traefik at http://localhost:8080/dashboard/#/http/routers/cadvisor@docker:</p> <pre><code>- \"traefik.http.middlewares.prefixstripper.stripprefix.prefixes=/cadvisor\"\n- \"traefik.http.routers.cadvisor.middlewares=prefixstripper\"\n</code></pre> Traefik added the `StripPrefix` middleware to our cAdvisor router <p>Let's try connecting to http://localhost/cadvisor again - the <code>404</code> persists. \ud83d\ude31 cAdvisor's behaviour didn't change - it still redirects to <code>/containers/</code> upon requests, thus messing up our routing instructions.</p> <p>What does work is connecting to http://localhost/cadvisor/containers/ directly. Traefik first matches the request to the <code>cadvisor</code> router, strips the prefix <code>/cadvisor</code> and forwards the <code>/containers/</code> request to cAdvisor. This one cAdvisor knows how to handle! So what are we missing?</p> <p>We already established that the problem arises upon redirects. Thus, we have two options:</p> <ol> <li>Always entering the full address to the site we want to access, e.g. http://localhost/cadvisor/containers/ (impractical) \ud83d\udc4e</li> <li>Rewriting redirects to the full path before they get sent to the client (better) \ud83d\udc4d</li> </ol> <p>Unfortunately, there exists no builtin middleware for rewriting redirects in Traefik, despite years-old requests to implement this feature. However, with the introduction of plugins for Traefik, the community took matters into their own hands and created several plugins for rewriting (Redirect) headers.</p> <p>In my case, the rewrite-headers plugin on GitHub did the trick. First, we have to install the plugin by adding the following lines to Traefik's <code>command</code> in our compose file:</p> <pre><code>- --experimental.plugins.rewriteHeaders.moduleName=github.com/XciD/traefik-plugin-rewrite-headers\n- --experimental.plugins.rewriteHeaders.version=v0.0.4\n</code></pre> <p>Then, we can add the following labels to cAdvisor's <code>labels</code> to use the freshly installed plugin as middleware:</p> <pre><code>- \"traefik.http.middlewares.cadvisor-redirect.plugin.rewriteHeaders.rewrites[0].header=Location\"\n- \"traefik.http.middlewares.cadvisor-redirect.plugin.rewriteHeaders.rewrites[0].regex=^(.+)$$\"\n- \"traefik.http.middlewares.cadvisor-redirect.plugin.rewriteHeaders.rewrites[0].replacement=/cadvisor$$1\"\n- \"traefik.http.routers.cadvisor.middlewares=auth,cadvisor-prefixstripper,cadvisor-redirect\"\n</code></pre> <p>With this configuration, every redirect header containing the <code>Location</code> field will have its contents modified, from .e.g <code>/containers/</code> to <code>/cadvisor/containers/</code>. Below is a schematic of the request flow:</p> Request flow with the rewrite-headers plugin <p>With this new middleware in place, we can finally access cAdvisor at http://localhost/cadvisor and it works as expected. Feel free to click around the web UI and see how the redirects get rewritten, always getting you to your desired destination.</p> <p>For completeness' sake, here's the full compose file with all the changes we made:</p> <pre><code>services:\n  traefik:\n    image: traefik:latest\n    container_name: traefik\n    command:\n      - --api.insecure=true\n      - --api.dashboard=true\n      - --providers.docker=true\n      - --entrypoints.web.address=:80\n      - --experimental.plugins.rewriteHeaders.moduleName=github.com/XciD/traefik-plugin-rewrite-headers\n      - --experimental.plugins.rewriteHeaders.version=v0.0.4\n    ports:\n      - 80:80\n      - 8080:8080\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n\n  grafana:\n    labels:\n      - traefik.http.routers.grafana.rule=PathPrefix(`/grafana`)\n      - traefik.http.routers.grafana.entrypoints=web\n      - traefik.enable=true\n    image: grafana/grafana:latest\n    environment:\n      - GF_SERVER_ROOT_URL=%(protocol)s://%(domain)s:%(http_port)s/grafana\n      - GF_SERVER_SERVE_FROM_SUB_PATH=true\n\n    ports:\n      - 3000:3000\n    container_name: grafana\n\n  cadvisor:\n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.http.routers.cadvisor.rule=PathPrefix(`/cadvisor`)\"\n      - \"traefik.http.routers.cadvisor.entrypoints=web\"\n      - \"traefik.http.middlewares.prefixstripper.stripprefix.prefixes=/cadvisor\"\n      - \"traefik.http.middlewares.cadvisor-redirect.plugin.rewriteHeaders.rewrites[0].header=Location\"\n      - \"traefik.http.middlewares.cadvisor-redirect.plugin.rewriteHeaders.rewrites[0].regex=^(.+)$$\"\n      - \"traefik.http.middlewares.cadvisor-redirect.plugin.rewriteHeaders.rewrites[0].replacement=/cadvisor$$1\"\n      - \"traefik.http.routers.cadvisor.middlewares=prefixstripper,cadvisor-redirect\"\n    image: gcr.io/cadvisor/cadvisor:v0.47.2\n    container_name: cadvisor\n    volumes:\n      - /:/rootfs:ro\n      - /var/run:/var/run:ro\n      - /sys:/sys:ro\n      - /var/lib/docker/:/var/lib/docker:ro\n      - /dev/disk/:/dev/disk:ro\n    devices:\n      - /dev/kmsg\n    privileged: true\n</code></pre>"},{"location":"blog/how-to-reverse-proxy-applications-on-subpaths-with-traefik/#tldr","title":"TL;DR","text":"<p>I learned quite a lot about Traefik and its available middlewares while trying to wrap my head around this problem. I hope this post helps you to understand how Traefik works and how to set it up to reverse-proxy applications on subpaths. There's basically two scenarios:</p> <ul> <li>You merely need to strip the prefix from the request before forwarding it to the application, either because your application can be configured to run on a subpath (e.g Grafana) or because it doesn't redirect/link anywhere else (e.g. some SPAs) . In this case, you can use the <code>StripPrefix</code> middleware.</li> <li>You need to rewrite redirects to the full path before sending them to the client, because your application redirects or links to other relative paths (e.g. cAdvisor). In this case, you can use the <code>rewrite-headers</code> plugin as shown above.</li> </ul>"},{"location":"blog/configuring-saml-authentication-for-omni-with-authentik/","title":"Configuring SAML Authentication for Omni with Authentik","text":"<p>A short write-up on how to configure SAML authentication for the Kubernetes management portal Omni using Authentik</p> <p>Omni is a Kubernetes management platform for Talos-based Kubernetes clusters that is available via a Business Source License which allows free installations in non-production environments. This makes it a perfect fit for powering my homelab's Kubernetes environment.</p> <p>When setting up, I noticed that Omni supports SAML-based authentication, but only provides documentation for a few selected providers:</p> <ul> <li>Auth0</li> <li>Workspace ONE</li> <li>Okta</li> <li>Entra ID</li> <li>Unify Identity Enterprise</li> <li>Keycloak</li> </ul> <p>I am running Authentik as IAM provider in my homelab, and thus had to do some research in order to find a working configuration. After talking to other community members in the Talos Slack community, I decided to document the needed steps.</p> <p>Warning</p> <p>I am not an expert on the SAML authentication protocol, and ended up with my current (working) setup by trial-and-error. Please follow this guide at your own risk, and second-guess the discussed configuration steps. Feel free to provide feedback in the comments.</p>"},{"location":"blog/configuring-saml-authentication-for-omni-with-authentik/#step-1-create-the-neccessary-property-mappings-for-omni-in-authentik","title":"Step 1: Create the Neccessary Property Mappings for Omni in Authentik","text":"<p>Omni utilizes a few specific property mappings from the IAM in use to manage users on its platform. I had to create these mappings from scratch, as I couldn't get the setup up and running with Authentik's default mappings.</p> <p>Create the following mappings from the Authentik admin interface (in <code>Customization &gt; Property Mappings</code>):</p> Email mapping: <p>Name: <code>email</code></p> <p>SAML Attribute Name: <code>email</code></p> <p>Friendly Name: <code>Email</code></p> <p>Expression: <code>return request.user.email</code></p> First Name mapping: <p>Name: <code>firstName</code></p> <p>SAML Attribute Name: <code>firstName</code></p> <p>Friendly Name: <code>First Name</code></p> <p>Expression: <code>return str.split(request.user.name, ' ')[0]</code></p> Last Name mapping: <p>Name: <code>lastName</code></p> <p>SAML Attribute Name: <code>lastName</code></p> <p>Friendly Name: <code>Last Name</code></p> <p>Expression: <code>return str.split(request.user.name, ' ')[1]</code></p>"},{"location":"blog/configuring-saml-authentication-for-omni-with-authentik/#step-2-configuring-a-saml-provider-for-omni-in-authentik","title":"Step 2: Configuring a SAML Provider for Omni in Authentik","text":"<p>First, a Provider for SAML authentication with Omni needs to be created from the Authentik admin interface.</p> <ol> <li>When asked for the provider's type, choose SAML Provider from the list</li> <li>Set the ACS URL to <code>https://&lt;your-omni-domain&gt;/saml/acs</code>.</li> <li>Set the Issuer to e.g. <code>authentik</code>.</li> <li>Set the Service Provider Binding to <code>Post</code>.</li> <li>In Advanced Protocol Settings, set the following:<ol> <li>Enable Sign assertions.</li> <li>Select the Property Mappings <code>email</code>, <code>firstName</code>, and <code>lastName</code> you created    in step 1.</li> <li>Set the NameID Property Mapping to <code>email</code>.</li> <li>Set the Digest Algorithm to <code>SHA256</code>.</li> <li>Set the Signature Algorithm to <code>RSA-SHA256</code>.</li> </ol> </li> </ol> <p>Save the provider, and open it in the Authentik admin interface. Copy the download URL from the Related objects section and note it down for configuring Omni later on. That's it.</p>"},{"location":"blog/configuring-saml-authentication-for-omni-with-authentik/#step-3-configuring-an-application-for-omni-in-authentik","title":"Step 3: Configuring an Application for Omni in Authentik","text":"<p>Next, an Application for SAML authentication with Omni needs to be created from the Authentik admin interface.</p> <ol> <li>Choose a name and slug that works for your setup.</li> <li>Select the provider you created in step 1.</li> <li>In the UI settings, set the following:<ol> <li>Set the Launch URL to <code>https://&lt;your-omni-domain&gt;</code></li> <li>Set the Icon to <code>https://raw.githubusercontent.com/siderolabs/omni/65244f67c7d8f30b7a146a48ab5514b39fd49d07/frontend/favicon.ico</code></li> </ol> </li> </ol> <p>Save the Application, that's it.</p>"},{"location":"blog/configuring-saml-authentication-for-omni-with-authentik/#step-4-configure-omni","title":"Step 4: Configure Omni","text":"<p>For configuring Omni, I mainly followed the official guide on running Omni on your own infrastructure.</p> <p>The following flags need to be adjusted:</p> <ul> <li><code>--auth-saml-enabled=true</code></li> <li><code>--auth-saml-url=&lt;authentik-metadata-url&gt;</code></li> </ul> <p>The <code>&lt;authentik-metadata-url&gt;</code> is the one you copied in step 2 after creating the provider.</p> <p>This is all that needs to be done for Omni to use Authentik for SAML authentication. Let me know if the setup works for you, and have fun exploring Omni!</p>"},{"location":"blog/three-cloud-native-projects-to-follow-in-2024/","title":"Three Cloud Native Projects to Follow in 2024","text":"<p>While 2023 is coming to an end, many people are already looking forward to 2024. People share predictions for the next year, forecasting emerging technologies and trends. This year, I want to jump on the bandwagon and share what I'll be looking at in 2024.</p> <p>Though I'm no fortune teller, thought leader, or expert (not even a senior while we're at it), 2024 is going to be my third year in the cloudnative space. I've been working with Kubernetes and related technologies for most of this time, got certified twice, and formed my own opinion(s) regarding many things in the ecosystem.</p> <p>Therefore, I want to share three projects that I think could become important to me in 2024.</p>"},{"location":"blog/three-cloud-native-projects-to-follow-in-2024/#leveraging-ai-for-kubernetes-operations","title":"Leveraging AI for Kubernetes Operations","text":"<p>The first project I want to talk about is k8sgpt. With version 0.1.0 released in late March this year, it has been accepted as a CNCF Sandbox project on December 19th, less than 9 months later.</p> <p>In what can be called the year of AI, it's no surprise that a project like this has experienced such a rapid growth and adoption, though it's still impressive: 55 contributors cut 43 releases over 863 commits so far, and the project has been starred 3.7k times to date.<sup>1</sup></p> k8sgpt analyzing a common ImagePullBackoff error. <p>The project's goal is to help operators and developers troubleshooting, securing, and optimizing their Kubernetes clusters. The project's CLI works with every CNCF-conformant Kubernetes distribution, and you can combine it with several different AI models depending on your use case, e.g. AzureOpenAI or Amazon SageMaker.</p> <p>I'm definitely looking forward to putting k8sgpt through its paces in 2024 should I come across a tough problem that I can't solve on my own, and I'm quite sure I'll like what I'll see.</p>"},{"location":"blog/three-cloud-native-projects-to-follow-in-2024/#no-more-yaml-an-alternative-to-helm","title":"No More YAML: An Alternative to Helm","text":"<p>Over the last two years, I deployed an awful lot of Helmcharts. Basically every project related to Kubernetes I explored had a <code>helm install</code> command in its README. While I'm not the biggest fan of YAML, I've always seen it as a necessary evil - there simply wasn't a better alternative in many cases.</p> <p>This might change in 2024, though. According to its website, Timoni is a new distribution and lifecycle management tool for cloud-native applications that aims to bring type safety, code generation, and validation features to our deployments.</p> <p>At the core of these efforts is CUElang, a language designed for exactly these use cases. It's a data definition language that allows you to define schemas and constraints for your data, and it's used to define what Timoni calls modules.</p> <p>From modules being distributed as OCI artifacts, to runtime secrets injection, to multi-cluster deployments, the project already has a lot to offer. I'm looking forward to seeing how it will evolve in 2024, and I'm excited to delve deeper into its applicable use cases and features than I did over the last months.</p> <p>Stay tuned for a more in-depth look at Timoni in the coming weeks!</p> Timoni's quickstart in action"},{"location":"blog/three-cloud-native-projects-to-follow-in-2024/#cloudnative-observability-with-ebpf","title":"Cloudnative Observability with eBPF","text":"<p>The last project I want to mention in this post is Tetragon by Isovalent, one of the initial developers of eBPF. Tetragon is a cloudnative observability platform that leverages eBPF to provide real-time and high-fidelity visibility into your Kubernetes clusters, Docker containers, and Linux hosts.</p> <p>While I've been a user of Cilium here and there (who isn't?), I've never really looked into eBPF or other projects by Isovalent so far. However, Tetragon seems promising: Having been released in version 1.0.0 on November 1st this year, it's a stable solution to track process executions, network connections, and system calls in your infrastructure - in real-time.</p> <p>Observability isn't the end of the road, though. Tetragon also allows you to enforce policies and block malicious activity in your infrastructure through CRDs called <code>TracingPolicies</code>.</p> Tetragon tracks command executions and results in real-time <p>For those of you who got curious about Tetragon, I recommend checking out one of the many available free labs by Isovalent, e.g. Getting Started with Tetragon - they're great, get you up and running in no time, and you will get a badge upon completion!</p>"},{"location":"blog/three-cloud-native-projects-to-follow-in-2024/#let-2024-come-im-ready","title":"Let 2024 Come - I'm Ready!","text":"<p>While I'll probably be drowning in bookmarks, tabs, and starred GitHub repositories by the end of January, I'm looking forward to exploring the projects above in 2024. Stay tuned for more in-depth posts on some of the projects and their respective features over the course of 2024, or go ahead and try them out yourself!</p> <ol> <li> <p>As of December 26th, 2023.\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/the-good-the-better-and-the-ugly---signing-git-commits/","title":"The Good, the Better, and the Ugly - Signing Git Commits","text":"<p>Git as a tool follows most of us around our day-to-day work, committing to different repositories in different organizations on different platforms like GitHub or GitLab.</p> <p>Being involved in various projects with a high cadence of commits every day makes it easier for bad actors to try and slip in nasty stuff - if we don't sign our commits, that is.</p> <p>Recently, a few stories about spoofed commits made it into my Twitter feed, namely by Eddie Jaoude and David Flanagan, aka rawkode:</p> <p>Eddie already hints at this blog post's topic in his tweet: Signing commits is important. But what had happened? A supposedly bad actor over at GitHub had committed to a project, hijacking Eddie's GitHub identity and making the commit look like Eddie himself had indeed created it. David replied, describing how he apparently managed to sneak a spoofed (non-malicious) commit into the codebase of VSCode. \ud83e\udd2f</p> <p>So, this way of spoofing commits is not only proven to work but also relatively easy to execute, and there's a ton of material online that explains precisely how to do it - even on Twitter. Time for us to act!</p>"},{"location":"blog/the-good-the-better-and-the-ugly---signing-git-commits/#what-does-signing-commits-mean","title":"What does 'signing commits' mean?","text":"<p>Forget about source code repositories and Git commits for a brief moment and think about your last (job) interview. You probably had a transcript of records from your university or maybe a letter of recommendation from your previous employer - but would your interviewer trust its contents without the signature of your university's chancellor or your former manager? - probably not!</p> <p>So why do we keep trusting unsigned commits? I don't know, but when asking around, it's because people either don't know about the features that GitHub and GitLab put at their disposal or get set back by the concept of signing keys.</p> <p>In the old days, when only the ugly method of signing commits was around (looking at you, GPG keys!), this was understandable. Anyways, there are more straightforward ways of signing your commits these days, and I'll try to give an overview of their advantages and shortcomings in the following paragraphs. Let's start with...</p>"},{"location":"blog/the-good-the-better-and-the-ugly---signing-git-commits/#the-good","title":"The Good","text":"<p>Picking up on the title of this blog, I'll start with the 'good' option. A good option for signing commits to me is defined by two qualities:</p> <ul> <li>ease of use</li> <li>availability to possible project members</li> </ul> <p>Ease of use in this context means that you don't need to read pages of docs just to be able to generate and maintain your signing keys. Availability means, that you don't need those keys at hand when creating a commit, an underrated quality in times of ephemeral developer environments like GitPod or CodeSpaces, in my opinion.</p> <p>Those qualities are where gitsign shines. It's a lightweight CLI that can sign your commits keyless, using your OIDC identity from GitHub, Google, or Microsoft. Installation via <code>brew</code> or <code>go install</code> is straightforward, and configuration for usage with Git can happen on a global or per-repository basis. For more information, head to the project's GitHub repository.</p> <p>Upon creating a commit, gitsign will create a unique authentication URL and write it to your CLI. If you follow this link, you will be taken to gitsign's authentication page, where you can choose from the three providers mentioned above. Once authenticated, the creation of your commit will commence, with a signature bearing your OIDC signature.</p> <p>The gitsign authentication challenge before and after successful authentication using sigstore.</p> <p>Let's inspect a commit signed this way:</p> <pre><code>$ git log --show-signature -1\ncommit b0d82aeb3ae2bb053542ebd306e135482152efae (HEAD -&gt; main)\ntlog index: 28881174\ngitsign: Signature made using certificate ID\n0xd02086681ee9733cf5abab19a08dcbebdd44cf82 |\nCN=sigstore-intermediate,O=sigstore.dev\ngitsign: Good signature from\n[dbodky@gmail.com](https://github.com/login/oauth)\nValidated Git signature: true\nValidated Rekor entry: true\nValidated Certificate claims: false\nWARNING: git verify-commit does not verify cert\nclaims. Prefer using `gitsign verify` instead.\nAuthor: Daniel Bodky &lt;dbodky@gmail.com&gt;\nDate:   Wed Jul 26 21:24:35 2023 +0200\n\n    Commit signed via gitsign\n</code></pre> <p>We see some output generated by gitsign: It displays the certificate ID and the CN generating it, as well as some information regarding the validation of the signature and its claims. It also prints a warning that it can't verify cert claims and that we should use gitsign verify instead. Let's do this next.</p> <p>gitsign will need two pieces of information from us: the email of the user's OIDC identity as well as the OIDC auth endpoint used - in this example, I chose GitHub:</p> <pre><code>$ gitsign verify --certificate-identity=dbodky@gmail.com \\\n--certificate-oidc-issuer=https://github.com/login/oauth\ntlog index: 28881174\ngitsign: Signature made using certificate ID\n0xd02086681ee9733cf5abab19a08dcbebdd44cf82 |\nCN=sigstore-intermediate,O=sigstore.dev\ngitsign: Good signature from\n[dbodky@gmail.com](https://github.com/login/oauth)\nValidated Git signature: true\nValidated Rekor entry: true\nValidated Certificate claims: true\n</code></pre> <p>We can observe a difference: This time, the certificate claims have been validated as well - we made sure that the signature has indeed been created by the entity contained in the signature. It does so by looking up the certificate on Rekor, which you can think of as a free, immutable database for signed metadata hosted by the cosign project.</p> <p>This approach is one of the disadvantages when using gitsign for signed commits: Platforms like GitHub or GitLab can't look up that information when displaying our commits. This means we will not get a shiny 'Verified' badge for our commits. If we wanted to test the integrity of our signatures, we would need to do so in a CI/CD pipeline, e.g. by leveraging the gitsign CLI within a job and failing upon unsuccessful verification.</p> Screenshot of GitHub's 'unverified' badge for a commit signed with gitsign <p>While the gitsign project states that it would like to work together with GitHub to provide proper signature verification in the future, this shortcoming is the reason why gitsign is just the good and not the best choice presented in this blog post, which brings us to...</p>"},{"location":"blog/the-good-the-better-and-the-ugly---signing-git-commits/#the-best","title":"The Best","text":"<p>I mentioned signing keys at the beginning of this post already, and the best method to sign commits right now (in my opinion) uses signing keys indeed, but no worries - most of you will be familiar with the concept already: it's SSH keys!</p> <p>Over the course of 2022, both GitHub and GitLab (in all tiers) announced the support of SSH keys for commit signing - a milestone! I added their respective announcements below for details.</p> <ul> <li> <p>GitLab 15.7 released introducing the GitLab CLI and with browser-based DAST GA</p> </li> <li> <p>SSH commit verification now supported</p> </li> </ul> <p>This means we can take the same key we already use for authenticating with the Git servers of GitHub/Lab and sign our commits with it, too! And different to gitsign signatures, both platforms will display a 'Verified' badge if we do so.</p> <p>In order for this to work, you will need to add your SSH public key to your account's list of verified signing keys:</p> <ul> <li>Adding an SSH signing key to your GitHub account</li> <li>Adding an SSH signing key to your GitLab account</li> </ul> <p>You also need to configure your global/local Git configuration to use this SSH key for signing your commits:</p> <pre><code>git config gpg.format ssh\ngit config user.signingkey /home/daniel/.ssh/githubkey\n</code></pre> <p>That's all! You can go on and sign your commits with one of your SSH keys now and let everyone know that it is indeed you who created the commit.</p> <p>Info</p> <p>You don't need to use the same keys for authentication and signing - just like you can use different keys to authenticate with different servers, you can use different keys for authenticating and signing.</p> <p>We looked at two out of three promised possibilities of signing your Git commits now, and we're left with...</p>"},{"location":"blog/the-good-the-better-and-the-ugly---signing-git-commits/#the-ugly","title":"The Ugly","text":"<p>After gitsign and SSH keys, let's look at GPG keys. Now, to make things clear from the start: ugly doesn't mean 'bad', and it's a personal opinion. After all, using GPG keys is natively supported by Git, just like SSH keys, and you will also end up with a 'Verified' badge.</p> <p>However, working with them always felt more clunky than using SSH keys for me, mainly because of the additional set of CLI commands you need for generating, managing, and distributing your keys. This adds cognitive load to the process of setting up and configuring your repositories and signing commits, and, speaking for myself, leads to a lot of googling whenever I need to export my GPG keys.</p> <p>If, for one reason or another, you want to use GPG keys, the process is similar to using SSH keys: First, create a GPG key and add it to your verified keys on GitHub/Lab:</p> <ul> <li>Adding a GPG key to your GitHub account</li> <li>Adding a GPG key to your GitLab account</li> </ul> <p>Then, again, you need to configure either your local or global Git settings to use the said key:</p> <pre><code>git config --unset gpg.format  # makes sure to use default format\ngit config user.signingkey XXXXXXXXXXXXXXXX\ngit config commit.gpgsign true\n</code></pre> <p>You can now use your GPG key(s) to sign commits in the same way as SSH keys.</p>"},{"location":"blog/the-good-the-better-and-the-ugly---signing-git-commits/#summing-it-up","title":"Summing it up","text":"<p>We looked at the good, the better, and the ugly in this blog post, and all of them got their justification(s) for being used when signing commits. While I see the potential for gitsign in CI/CD usage and custom policies, we only get proper integration with GitHub and GitLab for SSH/GPG keys (for now!).</p> <p>Below I created a small table comparing the key (missing) qualities of the methods introduced in this blog post.</p> SSH Key GPG Key gitsign Git Integration GitHub/Lab Integration Web/Chain of Trust No Additional Tooling <p>Web/Chain of trust is a quality that hasn't come up until now, but I wanted to add a few thoughts in the end - one of the core features of GPG keys is that they can be signed by others with their GPG keys. Over time, it's possible to establish a web of trust this way, with others confirming the authenticity of your key(s). SSH keys and gitsign can't provide such features.</p> <p>In cases where you just want to verify the authenticity of a commit e.g. from within your organization, this doesn't really matter, but it gets interesting when you want to go the extra mile and double-check a 'foreign' contributor:</p> <pre><code>curl https://github.com/&lt;username&gt;.gpg | gpg -v\n</code></pre> <p>This command will download all GPG signing keys the contributor added to their GitHub profile and inspect them (without importing them). All you need to do is find the key that shows up on the 'Verified' badge and have a look at its signatures - has it been signed by others? That's a good sign.</p>"},{"location":"homelab/","title":"Homelab","text":""},{"location":"homelab/docker/data-root/","title":"Change Docker's Data Directory","text":"<p>Because my TuringPi RK1s only have 32GB of storage space, Docker is configured to use an alternative <code>data-root</code> to store volumes, container runtime data, filesystems of the containers etc.</p> <p>For this, the NVMe SSD available for each Talos node is mounted at <code>/mnt/docker</code>.</p>"},{"location":"homelab/docker/data-root/#mounting-the-ssds","title":"Mounting the SSDs","text":"<p>The NVMe SSDs can be initially mounted like this:</p> <pre><code>lsblk\nNAME         MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS\nmmcblk0      179:0    0  29.1G  0 disk\n\u251c\u2500mmcblk0p1  179:1    0   512M  0 part /boot/firmware\n\u2514\u2500mmcblk0p2  179:2    0  28.6G  0 part /\nmmcblk0boot0 179:32   0     4M  1 disk\nmmcblk0boot1 179:64   0     4M  1 disk\nnvme0n1      259:0    0 931.5G  0 disk\n\u2514\u2500nvme0n1p1  259:1    0 931.5G  0 part\n\nmkdir -p /mnt/docker\nmount /dev/nvme0n1p1 /mnt/docker\n\nlsblk\nNAME         MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS\nmmcblk0      179:0    0  29.1G  0 disk\n\u251c\u2500mmcblk0p1  179:1    0   512M  0 part /boot/firmware\n\u2514\u2500mmcblk0p2  179:2    0  28.6G  0 part /\nmmcblk0boot0 179:32   0     4M  1 disk\nmmcblk0boot1 179:64   0     4M  1 disk\nnvme0n1      259:0    0 931.5G  0 disk\n\u2514\u2500nvme0n1p1  259:1    0 931.5G  0 part /mnt/docker\n</code></pre> <p>To persist this mount across reboots, restarts, etc., the configuration gets persisted in <code>/etc/fstab</code>:</p> <pre><code># /etc/fstab\nUUID=d19c17e0-6ac9-48a3-a116-3ffd1099d163 /mnt/docker   ext4    defaults        0       2\n</code></pre> <p>Partition UUID</p> <p>The partition UUID can be found using <code>blkid</code>:</p> <pre><code>blkid /dev/nvme0n1p1\n/dev/nvme0n1p1: UUID=\"d19c17e0-6ac9-48a3-a116-3ffd1099d163\" BLOCK_SIZE=\"4096\" TYPE=\"ext4\" PARTUUID=\"8f338872-881e-2e44-817a-680e84341a1c\"\n</code></pre> <p>Correctness of the changes can be tested by issuing <code>mount -a</code> before restarting the server to see if it actually works.</p>"},{"location":"homelab/docker/data-root/#configuring-docker","title":"Configuring Docker","text":"<p>Next, the Docker daemon's config can be configured to use this dedicated directory. It is located at <code>/etc/docker/daemon.json</code>.</p> <pre><code>{\n  \"data-root\": \"/mnt/docker\"\n}\n</code></pre> <p>Warning</p> <p>In some cases, <code>/etc/docker/daemon.json</code> doesn't exist and needs to be created first.</p> <p>Finally, the Docker daemon needs to be restarted to pick up on the changes:</p> <pre><code>sudo systemctl restart docker\ndocker info | grep Root\n  Docker Root Dir: /mnt/docker\n</code></pre>"},{"location":"homelab/docker/data-root/#moving-existing-data","title":"Moving Existing Data","text":"<p>In case you already ran Docker containers on the host before changing the <code>data-root</code> setting, you might probably want to move existing data over to the new directory, especially volumes.</p> <p>To do so, stop the Docker daemon, move the contents of <code>/var/lib/docker</code>, and start Docker again.</p> <pre><code>sudo systemctl stop docker\nsudo mv /var/lib/docker/* /mnt/docker\nsudo systemctl start docker\n</code></pre>"},{"location":"homelab/docker/expose-endpoint/","title":"Expose the Docker Endpoint","text":"<p>By default, the Docker daemon can be interacted with using a local socket at <code>/var/run/docker.sock</code>.</p> <p>In scenarios where applications external to the node need access to Docker (e.g. Portainer), a TCP endpoint can be configured in addition or as drop-in replacement, allowing for interaction with Docker over the network.</p>"},{"location":"homelab/docker/expose-endpoint/#preparations","title":"Preparations","text":"<p>Because Docker can do basically anything on a system, it's important to secure the TCP endpoint against unauthorized access. For doing so, Docker supports mTLS for authenticating parties involved in the communication.</p> <p>Thus, the following things are needed:</p> <ul> <li>a Certificate Authority (CA)</li> <li>Server Certificates for the Docker daemon(s)</li> <li>Client Certificates for applications communicating with the Docker daemon(s)</li> </ul>"},{"location":"homelab/docker/expose-endpoint/#creating-the-ca","title":"Creating the CA","text":"<p>Info</p> <p>In my homelab, I decided for the Docker CA to live on <code>olympus</code>, my management system.</p> <p>A CA can be bootstrapped like this:</p> <pre><code>mkdir /etc/docker/ca &amp;&amp; cd /etc/docker/ca\nopenssl genrsa -aes256 -out ca.key 4096\nopenssl req -new -x509 -days 1825 -key ca.key -sha256 -out ca.pem\n</code></pre> <p>This will create a CA key and certificate valid for 5 years. A secure passphrase should be chosen for the CA key. The information for the CA certificate can be answered to ones liking, but at least the common name (CN) should be something meaningful, e.g. <code>Docker CA</code>.</p>"},{"location":"homelab/docker/expose-endpoint/#creating-server-certificates","title":"Creating Server Certificates","text":"<p>Next, the created CA can be used to generate the server certificates for the Docker daemon(s).</p> <p>A server certificate can be created like this, e.g. for my Docker host <code>pandora</code>:</p> <pre><code>openssl genrsa -out pandora.key 4096\nopenssl req -subj \"/CN=pandora.bodkys.house\" -new -key pandora.key -out pandora.csr\necho 'subjectAltName = DNS:pandora.bodkys.house,IP:192.168.1.7,IP:127.0.0.1' &gt; pandora.cnf\necho 'extendedKeyUsage = serverAuth' &gt;&gt; pandora.cnf\nopenssl x509 -req -days 365 -sha256 -in pandora.csr \\\n  -CA ca.pem -CAkey ca.key -CAcreateserial \\\n  -out pandora.pem -extfile pandora.cnf\n</code></pre> <p>A few things are noteworthy here:</p> <ul> <li>the CN of the server certificate is set to pandora's fully qualified domain name (FQDN)</li> <li>the <code>subjectAltName</code> lists both, the FQDN as well as the IPs this machine is reachable on, as TCP connections can   communicate via FQDN or IP</li> <li>the <code>extendedKeyUsage</code> specifies that this certificate may only be used for server authentication</li> </ul>"},{"location":"homelab/docker/expose-endpoint/#creating-client-certificates","title":"Creating Client Certificates","text":"<p>Finally, the created CA can beb used to generate client certificates for applications connecting to the Docker daemon(s).</p> <p>A client certificate can be created like this, e.g. for Portainer managing another Docker host.</p> <pre><code>openssl genrsa -out portainer.key 4096\nopenssl req -subj '/CN=portainer.bodkys.house' -new -key portainer.key -out portainer.csr\necho 'extendedKeyUsage = clientAuth' &gt; portainer.cnf\nopenssl x509 -req -days 365 -sha256 -in portainer.csr \\\n  -CA ca.pem -CAkey ca.key -CAcreateserial \\\n  -out portainer.pem -extfile portainer.cnf\n</code></pre> <p>Again, a few things are noteworthy:</p> <ul> <li>the CN should be meaningful, but doesn't really matter here</li> <li>no <code>subjectAltNames</code> have been configured this time, as this certificate will never be used for serving an application</li> <li>the <code>extendedKeyUsage</code> has been set to <code>clientAuth</code> accordingly</li> </ul>"},{"location":"homelab/docker/expose-endpoint/#securing-certificates","title":"Securing Certificates","text":"<p>After creating the certificates, all of them should be secured accordingly to prevent extraction or manipulation:</p> <pre><code>chmod 0400 *.key\nchmod 0444 *.pem\n</code></pre>"},{"location":"homelab/docker/expose-endpoint/#configuring-the-docker-daemon","title":"Configuring the Docker Daemon","text":"<p>Now that the certificates are created, the Docker daemon(s) can be secured, using the server certificate(s).</p> <p>Move the CA cert as well as the Docker daemon's certificate keys and certs to the respective host(s) into <code>/etc/docker/certs</code>, and configure Docker like below (again, <code>pandora</code> is the example):</p> <pre><code>{\n  \"hosts\": [\"0.0.0.0:2376\", \"unix:///var/run/docker.sock\"],\n  \"tls\": true,\n  \"tlsverify\": true,\n  \"tlscacert\": \"/etc/docker/certs/ca.pem\",\n  \"tlscert\": \"/etc/docker/certs/pandora.pem\",\n  \"tlskey\": \"/etc/docker/certs/pandora.key\"\n}\n</code></pre> <p>Make sure to secure the copied certificates as described above.</p> <p>Configuration in <code>docker.service</code></p> <p>In my case, there was configuration in <code>docker.service</code> that clashed with the <code>hosts</code> setting in <code>/etc/docker/daemon.json</code>.</p> <p>I had to remove the <code>-H</code> flag/values in <code>/var/lib/systemd/system/docker.service</code> and reload the service file using <code>systemctl daemon-reload</code>.</p>"},{"location":"homelab/hardware/rk1/","title":"TuringPi RK1 SBC","text":"<p>Along with a few Raspberry Pis I am running TuringPi RK1 SBCs in my homelab. They sit on top of a shared cluster board, also manifactured by Turing Pi.</p> <p>They run either Talos Linux as Kubernetes nodes or a version of Ubuntu 24.04 LTS patched for the Rockchip as Docker host(s).</p>"},{"location":"homelab/hardware/rk1/#specifications","title":"Specifications","text":"Spec Value Notes Architecture <code>armv8</code> Cores 8 4x ARM Cortex-A76, 4x ARM Cortex-A55 Memory 32GB LPDDR4 eMMC Storage 32GB eMMC 5.1, SD 3.0 External Storage 1TB NVMe SSD"},{"location":"homelab/hardware/rk1/#machines","title":"Machines","text":"<p>The following machines are present in my homelab:</p> Hostname IP Usage <code>turing-1</code> <code>192.168.1.11</code> Talos Node <code>turing-2</code> <code>192.168.1.12</code> Talos Node <code>turing-3</code> <code>192.168.1.13</code> Talos Node <code>pandora</code> <code>192.168.1.7</code> Docker Host"},{"location":"homelab/hardware/rk1/#notes-and-misc","title":"Notes and Misc","text":""},{"location":"homelab/hardware/rk1/#setting-the-hostname","title":"Setting the Hostname","text":"<p>When installing Ubuntu 22.04 LTS using the image provided by TuringPi, the hostname will be <code>ubuntu</code> by default.</p> <p>It can be updated like this, e.g. for <code>pandora.bodkys.house</code>:</p> <pre><code>sudo hostnamectl set-hostname pandora.bodkys.house --static\nsudo hostnamectl set-hostname pandora-bodkys.house --transient\nsudo hostnamectl # check new config\n</code></pre> <p>In addition, the new hostname should be added to <code>/etc/hosts</code>:</p> <pre><code>127.0.0.1 pandora.bodkys.house\n</code></pre> <p>Finally, the <code>cloud-init</code> config needs updating by editing <code>/etc/cloud/cloud.cfg</code>:</p> <pre><code># /etc/cloud/cloud.cfg\npreserve_hostname: true # this will prevent cloud-init from restoring the old name\n</code></pre> <p>Changes can be validated by rebooting the machine.</p>"},{"location":"homelab/hardware/router/","title":"Router","text":"<p>As router I am using the FRITZ!Box 7583 provided by my ISP, as most German providers don't support BYOD very well.</p>"},{"location":"homelab/hardware/router/#specifications","title":"Specifications","text":"Spec Value Notes Uplink Speed <code>100Mbit/s</code> Downlink Speed <code>500Mbit/s</code> Due to bad wiring within the house, more like 380Mbit/s LAN Ports <code>5</code> 1Gbit/s speed on each port"},{"location":"homelab/hardware/router/#dhcp","title":"DHCP","text":"<p>The router takes care of DHCP within my intranet. Currently, there exist no separate networks e.g. for applications I run at home, IoT devices, or mobile devices for every day use.</p> <p>The settings for IPv4 are configured as below:</p> Setting Value Gateway Address <code>192.168.1.1</code> Subnet Mask <code>255.255.255.0</code> DHCP Address Pool <code>192.168.1.100</code> -&gt; <code>192.168.1.200</code> DHCP Lease Validity <code>10 days</code> <p>IPv6 Settings</p> <p>I currently do not actively use IPv6 within my homelab. Thus, the DHCP settings for IPv6 are left in their default state.</p>"},{"location":"homelab/hardware/router/#dns","title":"DNS","text":"<p>As I run AdGuard in my homelab, the router is configured to point to AdGuard as advertized DNS server. It is available at <code>192.168.1.3</code>, with the fallback set to Google's DNS service at <code>8.8.8.8</code>.</p>"},{"location":"homelab/hardware/router/#port-forwarding","title":"Port Forwarding","text":"<p>The router supports port forwarding for easier access to selected services from anywhere. Below is a list of ports currently forwarded:</p> Host Port Purpose <code>pandora</code> <code>25565</code> Minecraft Server"},{"location":"homelab/hardware/router/#notes-and-misc","title":"Notes and Misc","text":""},{"location":"homelab/hardware/rpi/","title":"Raspberry Pi 4B","text":"<p>Along with a few TuringPi RK1 SBCs I am running Raspberry Pi 4Bs in my homelab. I use them mostly as standalone Docker hosts.</p>"},{"location":"homelab/hardware/rpi/#specifications","title":"Specifications","text":"Spec Value Notes Architecture <code>armv8</code> Cores 4 Cortex-A72 @ 1.8GHz Memory 8GB LPDDR4 SD Storage varying varying"},{"location":"homelab/hardware/rpi/#machines","title":"Machines","text":"<p>The following machines are present in my homelab:</p> Hostname IP Usage <code>olympus</code> <code>192.168.1.3</code> Docker Host <code>rpi-2</code> not active not active <code>rpi-3</code> not active not active"},{"location":"homelab/hardware/rpi/#notes-and-misc","title":"Notes and Misc","text":"<p>TBA</p>"},{"location":"homelab/observability/alloy/","title":"Grafana Alloy","text":"<p>I use Grafana Alloy for collecting metrics, logs, and traces from different parts of my Homelab. Grafana Alloy is an open-source Open Telemetry Collector Distribution by Grafana which allows for easy integration with Grafana Cloud and many prebuilt configurations and integrations for common use-cases.</p> <p>As I am using the free tier of Grafana Cloud for my homelab's observability stack, it's a great fit.</p>"},{"location":"homelab/observability/alloy/#installing-grafana-alloy","title":"Installing Grafana Alloy","text":"<p>Grafana Alloy currently runs on the following machines in my homelab, with the listed integrations:</p> Node Integrations <code>olympus</code> Linux Node, Docker <code>pandora</code> Linux Node, Docker <p>The local instances are provisioned on Docker by Portainer.</p> <p>They onboard themselves into Grafana Cloud's fleet management feature for easy and centralized configuration.</p>"},{"location":"homelab/observability/alloy/#configuration","title":"Configuration","text":"<p>As mentioned above, all Grafana Alloy instances in my homelab are onboarded to Grafana Cloud's fleet management, which allows for easy discovery, management, and centralized/remote config management.</p> <p>For this, a minimal local configuration is available at <code>/etc/alloy/config.alloy</code> on each machine where Grafana Alloy is running.</p> <p>In addition, the environment variables <code>GCLOUD_RW_API_KEY</code> and <code>GCLOUD_FM_COLLETOR_ID</code> need to be set for each Grafana Alloy instance as they are used within the fleet management configuration. A reference Docker Compose stack can be found below.</p>"},{"location":"homelab/observability/alloy/#integrations","title":"Integrations","text":""},{"location":"homelab/observability/alloy/#linux-node","title":"Linux Node","text":"<p>For the Linux Node integration, no additional configuration needs to be done.</p>"},{"location":"homelab/observability/alloy/#docker-integration","title":"Docker Integration","text":"<p>For the Docker integration, Grafana Alloy needs to be able to run <code>cAdvisor</code> for monitoring container statistics. This requires the Grafana Alloy containers to run in <code>privileged</code> mode and mount a few specific volumes from the respective host:</p>"},{"location":"homelab/observability/alloy/#docker-compose-stack","title":"Docker Compose Stack","text":"<p>All Grafana Alloy instances utilize the same Docker Compose stack, with information like host names and instance IDs provided via environment variables within Portainer.</p> <p>Expand the annotations (1) for additional information for respective configuration options.</p> <ol> <li>This is an annotation!</li> </ol> <pre><code>services:\n  alloy:\n    command: &gt; # (1)!\n      run --server.http.listen.addr=0.0.0.0:12345\n      --storage.path==/var/lib/alloy/data\n      /etc/alloy/config.alloy\n    container_name: alloy\n    environment:\n      GCLOUD_RW_API_KEY: ${PORTAINER_GCLOUD_RW_API_KEY} # (2)!\n      GCLOUD_FM_COLLECTOR_ID: ${PORTAINER_HOST}.bodkys.house # (3)!\n    hostname: ${PORTAINER_HOST}.bodkys.house # (4)!\n    image: grafana/alloy:v1.8.1\n    labels:\n      traefik.docker.network: traefik_traefik # (5)!\n      traefik.enable: true\n      traefik.http.routers.alloy.entrypoints: websecure\n      traefik.http.routers.alloy.rule: Host(`alloy-${PORTAINER_HOST}.bodkys.house`)\n      traefik.http.routers.alloy.service: alloy\n      traefik.http.routers.alloy.tls.certResolver: civo\n      traefik.http.routers.alloy.tls.domains.0.main: alloy-${PORTAINER_HOST}.bodkys.house\n      traefik.http.services.alloy.loadbalancer.server.port: 12345\n      wud.link.template: https://github.com/grafana/alloy/releases/tag/$${transformed} # (6)!\n      wud.tag.include: ^v\\d+\\.\\d+\\.\\d+$$\n    networks:\n      - traefik_traefik\n    privileged: true # (7)!\n    restart: unless-stopped\n    volumes:\n      - /etc/alloy/config.alloy:/etc/alloy/config.alloy # (8)!\n      - alloy-data:/var/lib/alloy/data # (9)!\n      - /var/run/docker.sock:/var/run/docker.sock:ro # (10)!\n      - /:/rootfs:ro\n      - /var/run:/var/run:rw\n      - /sys:/sys:ro\n      - /mnt/docker/:/var/lib/docker:ro # (11)!\n\nnetworks:\n  traefik_traefik:\n    external: true\n\nvolumes:\n  alloy-data: {}\n</code></pre> <ol> <li>The listen address for Alloy needs to be set to <code>0.0.0.0</code> explicitly. Otherwise, the debug UI won't be reachable from     the outside.</li> <li>API key for Grafana Cloud Fleet Management, set in Portainer.</li> <li>Grafana Alloy instance ID, set to the respective host's FQDN in Portainer.</li> <li>Hostname needs to be set for the <code>constants.hostname</code> reference in the generated Grafana Alloy configuration     to work, as Docker containers use their container ID as hostname by default.</li> <li>Reverse-proxying via Traefik is configured via container labels.</li> <li>What's up Docker integration is configured via container labels.</li> <li>Needed for Grafana Alloy's cAdvisor integration.</li> <li>Contains the Grafana Cloud Fleet Management config.</li> <li>Contains Grafana Alloy's local data, e.g. WAL.</li> <li>All the mounts that follow are needed for Grafana Alloy's cAdvisor integration to work from inside the container.</li> <li>Custom Docker <code>data-root</code> directory, see Change Docker's data directory.</li> </ol>"},{"location":"homelab/talos/installation/","title":"Installation","text":"<p>Talos can be installed using the <code>talosctl</code> CLI and prebuilt images from factory.talos.dev, in my case for the TuringPi RK1 SBC.</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/category/homelab/","title":"Homelab","text":""},{"location":"blog/category/tutorials/","title":"Tutorials","text":""},{"location":"blog/category/cloud-native/","title":"Cloud Native","text":""},{"location":"blog/category/kubernetes/","title":"Kubernetes","text":""},{"location":"blog/author/https%3A/github.com/mocdaniel.png/","title":"Daniel Bodky","text":""}]}